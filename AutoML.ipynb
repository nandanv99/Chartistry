{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial steps\n",
    "- Handle Missing Values: Use strategies like mean or median imputation.\n",
    "- Encoding: Convert categorical variables into a format suitable for ML models, using techniques like one-hot encoding.\n",
    "- Feature Scaling: Normalize or standardize features.\n",
    "\n",
    "### What the below function is performing... \n",
    "- üß† _The data preprocessing function has successfully handled missing values (even though there weren't any in this sample data), encoded the categorical variables, and scaled the numerical variables. The data is also split into training and test sets._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_data(df, target_column):\n",
    "    \"\"\"\n",
    "    Preprocesses the data: handles missing values, encodes categorical variables, and scales numerical variables.\n",
    "    \n",
    "    Args:\n",
    "    - df (pandas.DataFrame): The input DataFrame.\n",
    "    - target_column (str): The target variable column name.\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test: Preprocessed data split into training and test sets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Separate target variable and features\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = [cname for cname in X.columns if \n",
    "                        X[cname].dtype == \"object\"]\n",
    "    numerical_cols = [cname for cname in X.columns if \n",
    "                      X[cname].dtype in ['int64', 'float64']]\n",
    "    \n",
    "    # Preprocessing for numerical data: imputation and scaling\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Preprocessing for categorical data: imputation and one-hot encoding\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Bundle preprocessing for numerical and categorical data\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ])\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # Preprocessing data using the defined transformers\n",
    "    X_train = preprocessor.fit_transform(X_train)\n",
    "    X_test = preprocessor.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Let's assume a sample dataset for demonstration (a more detailed dataset would be ideal for testing)\n",
    "sample_data = {\n",
    "    'Feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Feature2': ['A', 'B', 'A', 'B', 'A', 'A', 'B', 'B', 'A', 'B'],\n",
    "    'Target': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
    "}\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess_data(sample_df, 'Target')\n",
    "# X_train[:5], y_train[:5]  # Displaying the first 5 rows for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Model Selection and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial steps\n",
    "- Create a pool of candidate models.\n",
    "- Train each model on the dataset.\n",
    "- Evaluate each model's performance.\n",
    "- Rank models based on performance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precautions \n",
    "- For simplicity, let's consider three models: Logistic _Regression, Random Forest Classifier, and Gradient Boosting Classifier_ . We'll evaluate the performance based on accuracy.\n",
    "- All three models (Logistic Regression, Random Forest, and Gradient Boosting) have achieved an accuracy of 50% on the test set. This isn't surprising given the small and simplistic nature of the sample dataset. With more detailed and diverse data, we would expect to see differences in performance among the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Trains a set of models on the training data and evaluates them on the test data.\n",
    "    \n",
    "    Args:\n",
    "    - X_train, X_test, y_train, y_test: Training and test data.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary with model names as keys and their accuracy scores as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Random Forest': RandomForestClassifier(),\n",
    "        'Gradient Boosting': GradientBoostingClassifier()\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    model_scores = {}\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        model_scores[model_name] = accuracy\n",
    "    \n",
    "    return model_scores\n",
    "\n",
    "model_performance = evaluate_models(X_train, X_test, y_train, y_test)\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Hyperparameter Tuning\n",
    "_For the top-performing models, search for the best hyperparameters using techniques like grid search or random search._ \n",
    "- üëâüèª Hyperparameter tuning for just one of the models: the Random Forest Classifier. We'll use a basic grid search approach to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The hyperparameter tuning process suggests that the best parameters for the Random Forest model on this sample dataset are:\n",
    "\n",
    "- max_depth: 20\n",
    "- min_samples_leaf: 2\n",
    "- min_samples_split: 2\n",
    "- n_estimators: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def tune_random_forest(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning for the Random Forest model using GridSearchCV.\n",
    "    \n",
    "    Args:\n",
    "    - X_train, y_train: Training data.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Best hyperparameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define hyperparameters grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    rf_model = RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                               cv=3, n_jobs=-1, verbose=2)\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    return grid_search.best_params_\n",
    "\n",
    "# Due to the small size of the sample dataset, the grid search will be quick.\n",
    "best_params = tune_random_forest(X_train, y_train)\n",
    "best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Final Model Selection\n",
    "_With hyperparameter tuning complete, you can retrain your models using the best hyperparameters and then evaluate them on a separate validation set or use cross-validation to determine the final model._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
